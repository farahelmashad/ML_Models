{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075144ce",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 Logistic Regression — Complete Summary\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ✅ Intuition: What is Logistic Regression?\n",
    "\n",
    "- Logistic Regression is a **classification algorithm**, not a regression algorithm.\n",
    "- It models the **probability** that a given input belongs to **class 1**.\n",
    "- Output is a value **between 0 and 1**, representing the probability.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x + b)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488775c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. 📏 Decision Boundary\n",
    "\n",
    "- The linear part:  \n",
    "  $$\n",
    "  z = w^T x + b\n",
    "  $$\n",
    "- Apply the **sigmoid** to get a probability:  \n",
    "  $$\n",
    "  \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "- Classification rule:\n",
    "  - If \\( \\hat{y} \\geq 0.5 \\): Predict class **1**\n",
    "  - Else: Predict class **0**\n",
    "- The decision boundary is the line/plane where \\( w^T x + b = 0 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf6feb",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 🎯 What We're Trying to Learn\n",
    "\n",
    "- Find weights \\( w \\) and bias \\( b \\) that make \\( \\hat{y}^{(i)} \\) close to \\( y^{(i)} \\)\n",
    "- In other words: minimize the **difference** between predicted probabilities and true labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7212fb",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 📉 Loss Function: Log Loss (Binary Cross Entropy)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(i)} = -y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)}\n",
    "$$\n",
    "\n",
    "- Penalizes confident wrong predictions\n",
    "- Works well with sigmoid output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417296c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. 🔁 Training via Gradient Descent\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute predictions:\n",
    "   $$ \\hat{y} = \\sigma(w^T x + b) $$\n",
    "\n",
    "2. Compute error:\n",
    "   $$ \\text{error} = \\hat{y} - y $$\n",
    "\n",
    "3. Compute gradients:\n",
    "   - With respect to weights:\n",
    "     $$ \\frac{1}{m} X^T (\\hat{y} - y) $$\n",
    "   - With respect to bias:\n",
    "     $$ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "\n",
    "4. Update:\n",
    "   $$\n",
    "   w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "   $$\n",
    "   $$\n",
    "   b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "   $$\n",
    "\n",
    "Repeat until convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f40e5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. 🔮 Making Predictions\n",
    "\n",
    "```python\n",
    "z = X @ w + b\n",
    "pred_probs = sigmoid(z)\n",
    "preds = (pred_probs >= 0.5).astype(int)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3c954",
   "metadata": {},
   "source": [
    "\n",
    "## 7. 📦 Using Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8868683",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Summary Table\n",
    "\n",
    "| Concept            | Description |\n",
    "|--------------------|-------------|\n",
    "| Model              | \\( \\hat{y} = \\sigma(w^T x + b) \\) |\n",
    "| Sigmoid Function   | \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\) |\n",
    "| Loss Function      | Log Loss (Binary Cross Entropy) |\n",
    "| Gradient w.r.t `w` | \\( \\frac{1}{m} X^T(\\hat{y} - y) \\) |\n",
    "| Gradient w.r.t `b` | \\( \\frac{1}{m} \\sum (\\hat{y} - y) \\) |\n",
    "| Optimization       | Gradient Descent (no normal equation) |\n",
    "| Prediction Rule    | \\( \\hat{y} \\geq 0.5 \\Rightarrow 1 \\) else 0 |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}